{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":9835.268821,"end_time":"2024-11-29T15:18:37.899348","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-29T12:34:42.630527","version":"2.6.0"},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/Farama-Foundation/MAgent2","metadata":{"_uuid":"9f669fae-8bbf-4420-82db-a1d43583e10b","_cell_guid":"0ca7c070-3846-438e-a43d-3eb9c508e95d","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:58:30.899566Z","iopub.execute_input":"2024-12-15T20:58:30.899992Z","iopub.status.idle":"2024-12-15T20:59:25.443899Z","shell.execute_reply.started":"2024-12-15T20:58:30.899952Z","shell.execute_reply":"2024-12-15T20:59:25.441865Z"},"papermill":{"duration":33.272312,"end_time":"2024-11-29T12:35:19.024067","exception":false,"start_time":"2024-11-29T12:34:45.751755","status":"completed"},"scrolled":true,"tags":[],"jupyter":{"outputs_hidden":false},"id":"AsQFcJrPsSUb","outputId":"de21bff3-8e62-4e33-a934-73e181391eb8","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Import","metadata":{"_uuid":"16c64144-219c-481f-8ab0-f90a296adcf9","_cell_guid":"33b7d2f1-992b-499d-a66f-e17b8c33fc16","trusted":true,"collapsed":false,"papermill":{"duration":0.007022,"end_time":"2024-11-29T12:35:19.038519","exception":false,"start_time":"2024-11-29T12:35:19.031497","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"V6K8st1DsSUe"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\nfrom magent2.environments import battle_v4\nimport cv2\nfrom collections import deque\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"c883fe15-8130-45da-bc82-cee736639626","_cell_guid":"6e79a227-b85d-40ca-a3d1-7698943a7cce","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:25.446898Z","iopub.execute_input":"2024-12-15T20:59:25.447607Z","iopub.status.idle":"2024-12-15T20:59:31.564980Z","shell.execute_reply.started":"2024-12-15T20:59:25.447474Z","shell.execute_reply":"2024-12-15T20:59:31.563622Z"},"papermill":{"duration":4.648022,"end_time":"2024-11-29T12:35:23.693477","exception":false,"start_time":"2024-11-29T12:35:19.045455","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"po2mzBoFsSUg","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DQN","metadata":{"_uuid":"18082222-bd6e-474f-ac3a-099bedc5c252","_cell_guid":"6703be23-ffc9-4944-8e18-f400e2729588","trusted":true,"collapsed":false,"papermill":{"duration":0.006838,"end_time":"2024-11-29T12:35:23.918227","exception":false,"start_time":"2024-11-29T12:35:23.911389","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"qsvHiL9rsSUk"}},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape, device='cpu'):\n        super().__init__()\n        self.observation_shape = observation_shape\n        self.action_shape = action_shape\n        self.device = device\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], kernel_size=3),\n            nn.ReLU(),\n        )\n\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.network = nn.Sequential(\n            nn.Linear(flatten_dim, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, action_shape),\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.network(x)","metadata":{"_uuid":"0a512c1f-83c6-45ca-a415-1ee87d13f995","_cell_guid":"bcc09ca2-7170-4808-bd86-d1a66314a096","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.567208Z","iopub.execute_input":"2024-12-15T20:59:31.567858Z","iopub.status.idle":"2024-12-15T20:59:31.580141Z","shell.execute_reply.started":"2024-12-15T20:59:31.567810Z","shell.execute_reply":"2024-12-15T20:59:31.578096Z"},"papermill":{"duration":0.018182,"end_time":"2024-11-29T12:35:23.952303","exception":false,"start_time":"2024-11-29T12:35:23.934121","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"4uWvLuKvsSUk","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Replay Buffer","metadata":{"_uuid":"a05e439f-cda6-4eaf-8cfe-1dd28006bae2","_cell_guid":"5268250c-7bb4-4786-b50f-2b747dad0961","trusted":true,"collapsed":false,"papermill":{"duration":0.007009,"end_time":"2024-11-29T12:35:23.856757","exception":false,"start_time":"2024-11-29T12:35:23.849748","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"MgdKXxwCsSUh"}},{"cell_type":"markdown","source":"## Trainer","metadata":{"_uuid":"ea925093-c460-41c3-958a-9d168e3949d9","_cell_guid":"7a6b1f2b-5085-47e6-b902-b115008d6dd7","trusted":true,"collapsed":false,"papermill":{"duration":0.006993,"end_time":"2024-11-29T12:35:23.966416","exception":false,"start_time":"2024-11-29T12:35:23.959423","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"_DOd5Mg-sSUl"}},{"cell_type":"code","source":"class ReplayMemory(Dataset):\n    def __init__(self, maxlen,device = \"cpu\"):\n        super().__init__()\n        self.maxlen = maxlen\n        self.step_memory = [deque([],maxlen=self.maxlen)]\n        self.device = device\n\n    def push(self, step_idx, state, action, reward, next_state, done):\n        if step_idx == len(self.step_memory):\n            self.step_memory.append(deque([],maxlen=self.maxlen))\n        self.step_memory[step_idx].append((state, action, reward, next_state, done))\n\n    def __len__(self):\n        return sum([len(memory) for memory in self.step_memory])\n\n    def __getitem__(self, idx):\n        step_idx = 0\n        while idx >= len(self.step_memory[step_idx]):\n            idx -= len(self.step_memory[step_idx])\n            step_idx += 1\n        state, action, reward, next_state, done = self.step_memory[step_idx][idx]\n        return (\n            torch.Tensor(state).float().permute([2, 0, 1]).to(self.device),\n            torch.tensor(action).to(self.device),\n            torch.tensor(reward, dtype=torch.float).to(self.device),\n            torch.tensor(next_state).float().permute([2,0,1]).to(self.device),\n            torch.tensor(done, dtype=torch.float32).to(self.device),\n        )","metadata":{"_uuid":"dd69371c-5c95-47e2-8b92-d93980dcbd8b","_cell_guid":"3bb31d47-4f74-44a8-9980-752944ffb4b0","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.582722Z","iopub.execute_input":"2024-12-15T20:59:31.583287Z","iopub.status.idle":"2024-12-15T20:59:31.602829Z","shell.execute_reply.started":"2024-12-15T20:59:31.583240Z","shell.execute_reply":"2024-12-15T20:59:31.601041Z"},"papermill":{"duration":0.019958,"end_time":"2024-11-29T12:35:23.883672","exception":false,"start_time":"2024-11-29T12:35:23.863714","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"Lr6mxiaZsSUi","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self,\n        policy_dqn, target_dqn,\n        n_action,\n        loss_fn, optimizer, scheduler,\n        epsilon_start, epsilon_end, epsilon_decay,\n        device='cpu'\n    ):\n\n        self.policy_dqn = policy_dqn.to(device)\n        self.target_dqn = target_dqn.to(device)\n        self.target_dqn.eval()\n\n        self.n_action = n_action\n\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n        self.epsilon = self.epsilon_start\n\n        self.device = device\n\n        self.policy_dqn.apply(self.weights_init)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n            nn.init.xavier_uniform_(m.weight)\n            if torch.is_tensor(m.bias):\n                m.bias.data.fill_(0.01)\n\n\n    def policy(self, observation):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(low=0, high=self.n_action)\n        else:\n            with torch.no_grad():\n                q_values = self.policy_dqn(\n                    torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n                )\n            return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n\n    def optimize_model(self, replay_memory, batch_size, gamma):\n        if len(replay_memory) < batch_size:\n            return\n        train_loader = DataLoader(replay_memory, batch_size=batch_size, shuffle=True)\n        self.policy_dqn.train()\n\n        for observations, actions, rewards, next_observations, dones in train_loader:\n\n            self.policy_dqn.zero_grad()\n\n            observations = observations.to(device)\n            actions = actions.unsqueeze(1).to(device)\n            rewards = rewards.unsqueeze(1).to(device)\n            next_observations = next_observations.to(device)\n            dones = dones.unsqueeze(1).to(device)\n\n            current_q_values = self.policy_dqn(observations).gather(1, actions)\n\n            with torch.no_grad():\n                target_q_values = rewards + gamma * (1 - dones) * self.target_dqn(next_observations).max(1, keepdim=True)[0]\n\n            # Compute loss\n            loss = self.loss_fn(current_q_values, target_q_values)\n\n            # Optimize the network\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n\n    def train(self,\n              env, episodes,\n              target_agent, batch_size, gamma, replay_memory,\n              update_tg_freq, TAU\n             ):\n        train_rewards = []\n        train_durations = []\n\n        for episode in tqdm(range(episodes)):\n            ep_reward = 0\n\n            ep_steps = 0\n\n            observations = {}\n            actions = {}\n            step_idx = {}\n\n            env.reset()\n\n            for idx, agent in enumerate(env.agent_iter()):\n                ep_steps += 1\n                observation, reward, termination, truncation, info = env.last()\n\n                if target_agent in agent:\n                    ep_reward += reward\n                else:\n                    ep_reward -= abs(reward)\n\n                step_idx[agent] = 0\n                action = self.policy(observation)\n\n                observations[agent] = observation\n                actions[agent] = action\n                env.step(action)\n\n                if (idx+1) % env.num_agents == 0:\n                    break\n\n            for agent in env.agent_iter():\n                ep_steps += 1\n\n                next_observation, reward, termination, truncation, info = env.last()\n\n                if target_agent in agent:\n                    ep_reward += reward\n                else:\n                    ep_reward -= abs(reward)\n\n                # Agent die\n                if termination or truncation:\n                    action = None\n                else:\n                    action = self.policy(next_observation)\n\n                replay_memory.push(\n                    step_idx[agent],\n                    observations[agent],\n                    actions[agent],\n                    reward,\n                    next_observation,\n                    termination or truncation\n                )\n\n                step_idx[agent] += 1\n                observations[agent] = next_observation\n                actions[agent] = action\n                env.step(action)\n\n            # Training mô hình với memory hiện tại\n            self.optimize_model(replay_memory, batch_size, gamma)\n\n            # Cập nhật lại mô hình mục tiêu theo chu kì\n            if episode % update_tg_freq == 0:\n                target_dqn_state_dict = self.target_dqn.state_dict()\n                policy_dqn_state_dict = self.policy_dqn.state_dict()\n                for key in policy_dqn_state_dict:\n                    target_dqn_state_dict[key] = policy_dqn_state_dict[key]*TAU + target_dqn_state_dict[key]*(1-TAU)\n                self.target_dqn.load_state_dict(target_dqn_state_dict)\n\n\n            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n\n            print(f\"\\nEpisode {episode + 1}, Episode Reward: {ep_reward}, Steps: {ep_steps}, Epsilon: {self.epsilon}\")\n\n            train_rewards.append(ep_reward)\n            train_durations.append(ep_steps)\n\n        return train_rewards, train_durations","metadata":{"_uuid":"ce1cfb8d-3448-49b2-902b-8af2718ae2e4","_cell_guid":"f904b6bb-dd38-48de-b513-97fdce2f901f","execution":{"iopub.status.busy":"2024-12-15T20:59:31.606101Z","iopub.execute_input":"2024-12-15T20:59:31.606662Z","iopub.status.idle":"2024-12-15T20:59:31.638044Z","shell.execute_reply.started":"2024-12-15T20:59:31.606595Z","shell.execute_reply":"2024-12-15T20:59:31.636371Z"},"papermill":{"duration":0.037929,"end_time":"2024-11-29T12:35:24.011239","exception":false,"start_time":"2024-11-29T12:35:23.973310","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"trusted":true,"id":"LmFWyL_usSUl","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config","metadata":{"id":"xWMfYGAdsSUn"}},{"cell_type":"code","source":"\nenv = battle_v4.env(map_size=45, render_mode=\"rgb_array\",step_reward=-0.05, max_cycles = 300)\n\nepisodes = 50\ntarget_agent = 'blue'\nbatch_size = int(81 * episodes)\ngamma = 0.9\nupdate_tg_freq = 1\nTAU = 0.3\n\nmaxlen = 162 * episodes\n\nlearning_rate = 1e-3\ntheta = 0.000001\nepsilon_start = 1.0\nepsilon_end = 0.01\nepsilon_decay = 0.9\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.639435Z","iopub.execute_input":"2024-12-15T20:59:31.639909Z","iopub.status.idle":"2024-12-15T20:59:31.706802Z","shell.execute_reply.started":"2024-12-15T20:59:31.639865Z","shell.execute_reply":"2024-12-15T20:59:31.705116Z"},"id":"O-LNgyXrsSUo"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.708418Z","iopub.execute_input":"2024-12-15T20:59:31.708863Z","iopub.status.idle":"2024-12-15T20:59:31.725994Z","shell.execute_reply.started":"2024-12-15T20:59:31.708822Z","shell.execute_reply":"2024-12-15T20:59:31.724154Z"},"id":"ph8bg0yUsSUp"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training Loop","metadata":{"_uuid":"cc92f507-7093-427d-8a6f-0f361e37fd49","_cell_guid":"7440a5b8-bead-475e-8185-cfc9a78ef083","trusted":true,"collapsed":false,"papermill":{"duration":0.006842,"end_time":"2024-11-29T12:35:24.025631","exception":false,"start_time":"2024-11-29T12:35:24.018789","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"bsMx4KkzsSUp"}},{"cell_type":"code","source":"replay_memory = ReplayMemory(maxlen, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.728217Z","iopub.execute_input":"2024-12-15T20:59:31.728834Z","iopub.status.idle":"2024-12-15T20:59:31.740289Z","shell.execute_reply.started":"2024-12-15T20:59:31.728771Z","shell.execute_reply":"2024-12-15T20:59:31.738834Z"},"id":"tZd9oUAlsSUq"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"policy_dqn = QNetwork(env.observation_space(\"blue_0\").shape, env.action_space(\"blue_0\").n).to(device)\ntarget_dqn = QNetwork(env.observation_space(\"blue_0\").shape, env.action_space(\"blue_0\").n).to(device)\ntarget_dqn.load_state_dict(policy_dqn.state_dict())","metadata":{"_uuid":"dfa412e7-acdc-407f-992a-d83e68b91cc7","_cell_guid":"05b5ea12-a5db-44f7-a5ae-31cbda44a9bb","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.742189Z","iopub.execute_input":"2024-12-15T20:59:31.742679Z","iopub.status.idle":"2024-12-15T20:59:31.975304Z","shell.execute_reply.started":"2024-12-15T20:59:31.742630Z","shell.execute_reply":"2024-12-15T20:59:31.973858Z"},"papermill":{"duration":0.284104,"end_time":"2024-11-29T12:35:24.316848","exception":false,"start_time":"2024-11-29T12:35:24.032744","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"Hg8KvprxsSUr","outputId":"512be2cd-c60d-440d-ff7d-80baec6d03c0","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_function = nn.MSELoss()\noptimizer = torch.optim.AdamW(policy_dqn.parameters(), lr=learning_rate)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=episodes, eta_min=theta)","metadata":{"_uuid":"30db65e6-dc02-4555-94b0-336a2e2474ec","_cell_guid":"70b52fd4-7a08-4a94-8712-728182ce4420","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:31.977100Z","iopub.execute_input":"2024-12-15T20:59:31.977676Z","iopub.status.idle":"2024-12-15T20:59:33.554273Z","shell.execute_reply.started":"2024-12-15T20:59:31.977614Z","shell.execute_reply":"2024-12-15T20:59:33.552430Z"},"papermill":{"duration":1.152042,"end_time":"2024-11-29T12:35:25.479407","exception":false,"start_time":"2024-11-29T12:35:24.327365","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"dzXr2CaLsSUs","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrainer = Trainer(\n    policy_dqn, target_dqn,\n    env.action_space(\"red_0\").n,\n    loss_function, optimizer, lr_scheduler,\n    epsilon_start, epsilon_end, epsilon_decay,\n    device=device\n)","metadata":{"_uuid":"183cf9b6-2ea5-4037-a206-777dc6b86694","_cell_guid":"58579552-2c51-41e2-8961-08b83c5bc5f6","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:33.556055Z","iopub.execute_input":"2024-12-15T20:59:33.556659Z","iopub.status.idle":"2024-12-15T20:59:33.573938Z","shell.execute_reply.started":"2024-12-15T20:59:33.556616Z","shell.execute_reply":"2024-12-15T20:59:33.572283Z"},"papermill":{"duration":0.054528,"end_time":"2024-11-29T12:35:25.541611","exception":false,"start_time":"2024-11-29T12:35:25.487083","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"JuZgCRyCsSUs","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntrain_rewards, train_durations = trainer.train(\n    env, episodes,\n    target_agent, batch_size, gamma, replay_memory,\n    update_tg_freq, TAU\n)","metadata":{"_uuid":"c24dc367-bdb4-4662-8c06-ef3c1dd41c98","_cell_guid":"0ca0afe2-6b9e-4735-8843-eecef7f49550","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T20:59:33.575854Z","iopub.execute_input":"2024-12-15T20:59:33.576492Z","iopub.status.idle":"2024-12-15T22:53:26.595791Z","shell.execute_reply.started":"2024-12-15T20:59:33.576382Z","shell.execute_reply":"2024-12-15T22:53:26.594304Z"},"papermill":{"duration":9702.396059,"end_time":"2024-11-29T15:18:25.778310","exception":false,"start_time":"2024-11-29T12:36:43.382251","status":"completed"},"tags":[],"jupyter":{"outputs_hidden":false},"id":"3INdaN92sSUt","outputId":"7ef9a0b1-1afe-4e86-aa95-e000cf728c54","collapsed":false},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.policy_dqn","metadata":{"_uuid":"20340d54-e379-4658-8966-10aba213d4b4","_cell_guid":"319983ee-abaa-4bf2-914b-fdcf21d92f88","trusted":true,"papermill":{"duration":0.025807,"end_time":"2024-11-29T15:18:29.260902","exception":false,"start_time":"2024-11-29T15:18:29.235095","status":"completed"},"scrolled":true,"tags":[],"jupyter":{"outputs_hidden":false},"id":"duJaLq6LsSUt","outputId":"3921aef2-f425-4a7f-eae8-45173a7dcd68","collapsed":false,"execution":{"iopub.status.busy":"2024-12-15T22:53:26.597870Z","iopub.execute_input":"2024-12-15T22:53:26.598452Z","iopub.status.idle":"2024-12-15T22:53:26.608424Z","shell.execute_reply.started":"2024-12-15T22:53:26.598391Z","shell.execute_reply":"2024-12-15T22:53:26.606825Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(trainer.policy_dqn.state_dict(), 'blue.pt')","metadata":{"trusted":true,"id":"xq5eaQO5sSUt","execution":{"iopub.status.busy":"2024-12-15T22:53:26.613130Z","iopub.execute_input":"2024-12-15T22:53:26.613587Z","iopub.status.idle":"2024-12-15T22:53:26.638990Z","shell.execute_reply.started":"2024-12-15T22:53:26.613511Z","shell.execute_reply":"2024-12-15T22:53:26.637733Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Eval Model","metadata":{}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/giangbang/RL-final-project-AIT-3007\n%cd RL-final-project-AIT-3007","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:21:57.528557Z","iopub.execute_input":"2024-12-15T23:21:57.529339Z","iopub.status.idle":"2024-12-15T23:22:01.987704Z","shell.execute_reply.started":"2024-12-15T23:21:57.529276Z","shell.execute_reply":"2024-12-15T23:22:01.985511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:22:46.809182Z","iopub.execute_input":"2024-12-15T23:22:46.809800Z","iopub.status.idle":"2024-12-15T23:23:28.644464Z","shell.execute_reply.started":"2024-12-15T23:22:46.809749Z","shell.execute_reply":"2024-12-15T23:23:28.642278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from magent2.environments import battle_v4\nfrom torch_model import QNetwork\nfrom final_torch_model import QNetwork as FinalQNetwork\nimport torch\nimport numpy as np\n\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n\n\ndef eval():\n    max_cycles = 300\n    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    def random_policy(env, agent, obs):\n        return env.action_space(agent).sample()\n\n    q_network = QNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    q_network.load_state_dict(\n        torch.load(\"red.pt\", weights_only=True, map_location=\"cpu\")\n    )\n    q_network.to(device)\n\n    final_q_network = FinalQNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    final_q_network.load_state_dict(\n        torch.load(\"red_final.pt\", weights_only=True, map_location=\"cpu\")\n    )\n    final_q_network.to(device)\n# blue policy\n    blue_q_network = QNetwork(\n        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n    blue_q_network.load_state_dict(\n        torch.load(\"/kaggle/working/blue.pt\", weights_only=True, map_location=\"cpu\")\n    )\n    blue_q_network.to(device)\n    \n    def blue_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.no_grad():\n            q_values = blue_q_network(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n    def pretrain_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.no_grad():\n            q_values = q_network(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n    def final_pretrain_policy(env, agent, obs):\n        observation = (\n            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.no_grad():\n            q_values = final_q_network(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n        red_win, blue_win = [], []\n        red_tot_rw, blue_tot_rw = [], []\n        n_agent_each_team = len(env.env.action_spaces) // 2\n\n        step_game = []\n\n        for _ in tqdm(range(n_episode)):\n            env.reset()\n            n_kill = {\"red\": 0, \"blue\": 0}\n            red_reward, blue_reward = 0, 0\n            total_step = 0\n\n            for agent in env.agent_iter():\n                observation, reward, termination, truncation, info = env.last()\n                agent_team = agent.split(\"_\")[0]\n\n                n_kill[agent_team] += (\n                    reward > 4.5\n                )  # This assumes default reward settups\n                if agent_team == \"red\":\n                    red_reward += reward\n                else:\n                    blue_reward += reward\n\n                if termination or truncation:\n                    action = None  # this agent has died\n                else:\n                    if agent_team == \"red\":\n                        action = red_policy(env, agent, observation)\n                    else:\n                        action = blue_policy(env, agent, observation)\n\n                env.step(action)\n                \n                if not termination and not truncation:\n                    total_step += 1\n            \n            step_game.append(total_step)\n\n            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n            red_win.append(who_wins == \"red\")\n            blue_win.append(who_wins == \"blue\")\n\n            red_tot_rw.append(red_reward / n_agent_each_team)\n            blue_tot_rw.append(blue_reward / n_agent_each_team)\n\n        return {\n            \"winrate_red\": np.mean(red_win),\n            \"winrate_blue\": np.mean(blue_win),\n            \"average_rewards_red\": np.mean(red_tot_rw),\n            \"average_rewards_blue\": np.mean(blue_tot_rw),\n            \"average_step\": int(np.mean(step_game)),\n        }\n\n    print(\"=\" * 20)\n    print(\"Eval with random policy\")\n    print(\n        run_eval(\n            env=env, red_policy=random_policy, blue_policy=blue_policy, n_episode=100\n        )\n    )\n    print(\"=\" * 20)\n\n    print(\"Eval with trained policy\")\n    print(\n        run_eval(\n            env=env, red_policy=pretrain_policy, blue_policy=blue_policy, n_episode=100\n        )\n    )\n    print(\"=\" * 20)\n\n    print(\"Eval with final trained policy\")\n    print(\n        run_eval(\n            env=env,\n            red_policy=final_pretrain_policy,\n            blue_policy=blue_policy,\n            n_episode=100,\n        )\n    )\n    print(\"=\" * 20)\n\n\nif __name__ == \"__main__\":\n    eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T23:32:08.778115Z","iopub.execute_input":"2024-12-15T23:32:08.778704Z","iopub.status.idle":"2024-12-15T23:35:24.840958Z","shell.execute_reply.started":"2024-12-15T23:32:08.778657Z","shell.execute_reply":"2024-12-15T23:35:24.839822Z"}},"outputs":[],"execution_count":null}]}