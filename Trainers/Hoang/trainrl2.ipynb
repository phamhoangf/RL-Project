{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/Farama-Foundation/MAgent2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qJfbxkLbg982","outputId":"2432f192-94ad-42ee-a695-ec90e05d9dea","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:14.839219Z","iopub.execute_input":"2024-12-04T12:20:14.839583Z","iopub.status.idle":"2024-12-04T12:20:48.282665Z","shell.execute_reply.started":"2024-12-04T12:20:14.839522Z","shell.execute_reply":"2024-12-04T12:20:48.281622Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Farama-Foundation/MAgent2\n  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-a0qxyxij\n  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-a0qxyxij\n  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\nCollecting pygame>=2.1.0 (from magent2==0.3.3)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\nRequirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.0.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: magent2\n  Building wheel for magent2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1696104 sha256=a8ee1b4680200be62223aaa96cd5fef0d76dd2d6f247c6affba23e680b09f7b9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mk5x6kf9/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\nSuccessfully built magent2\nInstalling collected packages: pygame, magent2\nSuccessfully installed magent2-0.3.3 pygame-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from collections import deque, namedtuple","metadata":{"id":"Yh3GxMBOCAMr","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:48.284330Z","iopub.execute_input":"2024-12-04T12:20:48.284648Z","iopub.status.idle":"2024-12-04T12:20:48.289015Z","shell.execute_reply.started":"2024-12-04T12:20:48.284618Z","shell.execute_reply":"2024-12-04T12:20:48.288188Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# DQN NETWORK\n","metadata":{"id":"nhG1xHhXUyee"}},{"cell_type":"code","source":"\nimport os\nfrom magent2.environments import battle_v4\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport random\nimport numpy as np\nimport cv2\n\nimport math\nimport matplotlib.pyplot as plt","metadata":{"id":"1O8rKufghFKE","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:48.290114Z","iopub.execute_input":"2024-12-04T12:20:48.290442Z","iopub.status.idle":"2024-12-04T12:20:51.783741Z","shell.execute_reply.started":"2024-12-04T12:20:48.290405Z","shell.execute_reply":"2024-12-04T12:20:51.783007Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n            nn.ReLU(),\n        )\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.network = nn.Sequential(\n            nn.Linear(flatten_dim, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, action_shape),\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.network(x)","metadata":{"id":"z5Jf8moJBh5n","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.786196Z","iopub.execute_input":"2024-12-04T12:20:51.787062Z","iopub.status.idle":"2024-12-04T12:20:51.794999Z","shell.execute_reply.started":"2024-12-04T12:20:51.787020Z","shell.execute_reply":"2024-12-04T12:20:51.794227Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Replay Memory","metadata":{"id":"Dx94CHtWDOdf"}},{"cell_type":"code","source":"Transition = namedtuple('Transition',\n                        ('observation', 'action', 'reward', 'next_observation', 'done'))","metadata":{"id":"jefwoyizrCpe","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.796340Z","iopub.execute_input":"2024-12-04T12:20:51.796627Z","iopub.status.idle":"2024-12-04T12:20:51.808952Z","shell.execute_reply.started":"2024-12-04T12:20:51.796601Z","shell.execute_reply":"2024-12-04T12:20:51.808265Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Define memory for Experience Replay\nclass ReplayMemory(object):\n    def __init__(self, maxlen):\n        self.memory = deque([], maxlen=maxlen)\n\n    def append(self, *args):\n        self.memory.append(Transition(*args))\n\n    def sample(self, sample_size):\n        return random.sample(self.memory, sample_size)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"id":"wrR8DKyMDN6q","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.809925Z","iopub.execute_input":"2024-12-04T12:20:51.810167Z","iopub.status.idle":"2024-12-04T12:20:51.819992Z","shell.execute_reply.started":"2024-12-04T12:20:51.810143Z","shell.execute_reply":"2024-12-04T12:20:51.819199Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# Training","metadata":{"id":"Cw9lLvijVJdQ"}},{"cell_type":"code","source":"# Hyperparameters\nlearning_rate = 0.0003\ngamma = 0.9\nepsilon_start = 0.9\nepsilon_end = 0.05\nnetwork_sync_rate = 20\nepsilon_decay = 150000\nbatch_size = 128\nepisodes = 20\nTAU = 0.005","metadata":{"id":"woLb6sWfH6eh","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.821254Z","iopub.execute_input":"2024-12-04T12:20:51.821616Z","iopub.status.idle":"2024-12-04T12:20:51.829730Z","shell.execute_reply.started":"2024-12-04T12:20:51.821576Z","shell.execute_reply":"2024-12-04T12:20:51.829029Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EqDOslDfJYvP","outputId":"93ebf1f4-94de-4e36-e05e-30d916f7b07e","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.830610Z","iopub.execute_input":"2024-12-04T12:20:51.830852Z","iopub.status.idle":"2024-12-04T12:20:51.872659Z","shell.execute_reply.started":"2024-12-04T12:20:51.830827Z","shell.execute_reply":"2024-12-04T12:20:51.871757Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Epsilon-Greedy","metadata":{"id":"JK_htOQcHeQw"}},{"cell_type":"code","source":"steps_done = 0\ndef policy(observation, agent, env, q_network, device):\n    global steps_done\n    sample = random.random()\n    epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n        math.exp(-1. * steps_done / epsilon_decay)\n    steps_done += 1\n    if sample < epsilon:\n        return env.action_space(agent).sample()\n    else:\n        observation = (\n            torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n        )\n        with torch.no_grad():\n            q_values = q_network(observation)\n        return torch.argmax(q_values, dim=1).cpu().numpy()[0]","metadata":{"id":"manztjcXHg6y","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.873928Z","iopub.execute_input":"2024-12-04T12:20:51.874337Z","iopub.status.idle":"2024-12-04T12:20:51.885974Z","shell.execute_reply.started":"2024-12-04T12:20:51.874298Z","shell.execute_reply":"2024-12-04T12:20:51.885270Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Init environment and network","metadata":{}},{"cell_type":"code","source":"# Initialize environment and network\nenv = battle_v4.env(map_size=45, minimap_mode=False, step_reward=-0.005,\ndead_penalty=-0.1, attack_penalty=-0.005, attack_opponent_reward=0.05,\nmax_cycles=300, extra_features=False)\nenv.reset()\n\npolicy_dqn = QNetwork(\n    observation_shape=env.observation_space(\"blue_0\").shape,\n    action_shape=env.action_space(\"blue_0\").n,\n)\npolicy_dqn = policy_dqn.to(device)\n\ntarget_dqn = QNetwork(\n    observation_shape=env.observation_space(\"blue_0\").shape,\n    action_shape=env.action_space(\"blue_0\").n,\n)\ntarget_dqn = target_dqn.to(device)\n\n\n# Make the target and policy networks the same (copy weights/biases from one network to the other)\ntarget_dqn.load_state_dict(policy_dqn.state_dict())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MUJUa_AaJt6d","outputId":"0ad1e0de-12bd-4b55-9ae2-36a4296d67cf","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:51.888082Z","iopub.execute_input":"2024-12-04T12:20:51.888329Z","iopub.status.idle":"2024-12-04T12:20:52.192261Z","shell.execute_reply.started":"2024-12-04T12:20:51.888304Z","shell.execute_reply":"2024-12-04T12:20:52.191295Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"\noptimizer = optim.Adam(policy_dqn.parameters(), lr=learning_rate)                # NN Optimizer.","metadata":{"id":"y8rtR-8fLDD5","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:52.193544Z","iopub.execute_input":"2024-12-04T12:20:52.194143Z","iopub.status.idle":"2024-12-04T12:20:53.011000Z","shell.execute_reply.started":"2024-12-04T12:20:52.194094Z","shell.execute_reply":"2024-12-04T12:20:53.010040Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"loss_fn = nn.SmoothL1Loss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:53.012097Z","iopub.execute_input":"2024-12-04T12:20:53.012467Z","iopub.status.idle":"2024-12-04T12:20:53.016812Z","shell.execute_reply.started":"2024-12-04T12:20:53.012440Z","shell.execute_reply":"2024-12-04T12:20:53.015763Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Training Loop","metadata":{"id":"XzKFU6dlM5YW"}},{"cell_type":"code","source":"replay_buffer = ReplayMemory(10000)","metadata":{"id":"QWpCY1Ri6Rbd","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:53.017891Z","iopub.execute_input":"2024-12-04T12:20:53.018149Z","iopub.status.idle":"2024-12-04T12:20:53.028492Z","shell.execute_reply.started":"2024-12-04T12:20:53.018124Z","shell.execute_reply":"2024-12-04T12:20:53.027763Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def optimize_model(policy_dqn, target_dqn):\n    if len(replay_buffer) < batch_size:\n        return\n    transitions = replay_buffer.sample(batch_size)\n\n    batch = Transition(*zip(*transitions))\n\n    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n                                            batch.next_observation)), device=device, dtype=torch.bool)\n    non_final_next_observations = torch.cat([s for s in batch.next_observation if s is not None])\n\n    observation_batch = torch.cat(batch.observation).to(device)\n    action_batch = torch.cat(batch.action).to(device)\n    reward_batch = torch.cat(batch.reward).to(device)\n\n\n    state_action_values = policy_dqn(observation_batch).gather(1, action_batch)\n\n    next_state_values = torch.zeros(batch_size, device=device)\n    with torch.no_grad():\n        next_state_values[non_final_mask] = target_dqn(non_final_next_observations).max(1).values\n\n    expected_state_action_values = (next_state_values * gamma) + reward_batch\n\n    loss = loss_fn(state_action_values, expected_state_action_values.unsqueeze(1))\n    \n    optimizer.zero_grad()\n    loss.backward()\n    torch.nn.utils.clip_grad_value_(policy_dqn.parameters(), 100)\n    optimizer.step()\n\n","metadata":{"id":"QmM8xwWI6JPI","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:53.029427Z","iopub.execute_input":"2024-12-04T12:20:53.029712Z","iopub.status.idle":"2024-12-04T12:20:53.039380Z","shell.execute_reply.started":"2024-12-04T12:20:53.029686Z","shell.execute_reply":"2024-12-04T12:20:53.038676Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"\n\n\n\ndef train_q_network(env, policy_dqn, target_dqn, optimizer, loss_fn, device):\n    \"\"\"\n    Training loop for Q-learning without a replay buffer.\n\n    \"\"\"\n    episode_rewards = []\n    # List to keep track of epsilon decay\n    epsilon_history = []\n\n    try:\n        for episode in range(episodes):\n            env.reset()\n            ep_reward = 0\n            # ep_loss = 0\n            ep_steps = 0\n            for agent in env.agent_iter():\n\n                observation, reward, termination, truncation, info = env.last()\n\n                if termination or truncation:\n                    action = None  # Agent is dead\n                    env.step(action)\n                else:\n                    agent_handle = agent.split(\"_\")[0]\n                    if agent_handle == \"blue\":\n                        # Get action\n                        action = blue_policy(observation, agent, env, policy_dqn, device)\n\n                        # Take action\n                        env.step(action)\n\n                        # Get next state information\n                        next_observation, reward, termination, truncation, info = env.last()\n\n                        ep_reward += reward\n\n\n                        # Append transition to replay buffer\n                        observation = torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n                        next_observation = torch.Tensor(next_observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n                        action = torch.tensor([action], device=device, dtype=torch.int64).unsqueeze(0)\n                        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n                        replay_buffer.append(observation, action, reward, next_observation, termination)\n\n                    else:\n                        # Random policy for red team\n                        action = env.action_space(agent).sample()\n                        env.step(action)\n\n                        # Get next state information\n                        next_observation, reward, termination, truncation, info = env.last()\n\n                        # ep_reward -= reward\n\n                        # Append transition to replay buffer\n                        observation = torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n                        next_observation = torch.Tensor(next_observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n                        action = torch.tensor([action], device=device, dtype=torch.int64).unsqueeze(0)\n                        reward = torch.tensor([reward], device=device, dtype=torch.float32)\n                        replay_buffer.append(observation, action, reward, next_observation, termination)\n\n                        \n                    # Optimize Model\n                    optimize_model(policy_dqn, target_dqn)\n\n                    ep_steps += 1\n                    if ep_steps % network_sync_rate == 0:\n                        target_net_state_dict = target_dqn.state_dict()\n                        policy_net_state_dict = policy_dqn.state_dict()\n                        for key in policy_net_state_dict:\n                            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n                        target_dqn.load_state_dict(target_net_state_dict)\n\n            # Decay epsilon\n            epsilon = epsilon_end + (epsilon_start - epsilon_end) * \\\n                math.exp(-1. * steps_done / epsilon_decay)\n            epsilon_history.append(epsilon)\n\n            # Record episode statistics\n            episode_rewards.append(ep_reward)\n\n            print(f\"Episode {episode+1}/{episodes} end after {ep_steps} steps -Reward: {ep_reward} -Epsilon: {epsilon}\")\n\n        # Save trained model\n        torch.save({\n            'model_state_dict': policy_dqn.state_dict(),\n        }, \"blue_agent.pt\")\n        print(\"Model saved as 'blue_agent.pt'\")\n        return episode_rewards\n\n    except KeyboardInterrupt:\n        print(\"\\nTraining interrupted. Saving model checkpoint...\")\n        torch.save({\n            'model_state_dict': policy_dqn.state_dict(),\n        }, \"blue_agent_interrupted.pt\")\n        print(\"Model saved as 'blue_agent_interrupted.pt'\")\n        return episode_rewards","metadata":{"id":"7dPMsF2SLV8C","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:53.040628Z","iopub.execute_input":"2024-12-04T12:20:53.040973Z","iopub.status.idle":"2024-12-04T12:20:53.056601Z","shell.execute_reply.started":"2024-12-04T12:20:53.040936Z","shell.execute_reply":"2024-12-04T12:20:53.055686Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"## Training Log","metadata":{"id":"I0nb1ZJLFrr0"}},{"cell_type":"code","source":"# Train the network\nepisode_rewards = train_q_network(env, policy_dqn, target_dqn, optimizer, loss_fn, device)","metadata":{"id":"5QF_uGs-tgPZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"79137ce3-0b16-4613-db0a-92fb72480ef4","trusted":true,"execution":{"iopub.status.busy":"2024-12-04T12:20:53.057655Z","iopub.execute_input":"2024-12-04T12:20:53.057942Z","iopub.status.idle":"2024-12-04T13:22:14.311988Z","shell.execute_reply.started":"2024-12-04T12:20:53.057916Z","shell.execute_reply":"2024-12-04T13:22:14.311112Z"}},"outputs":[{"name":"stdout","text":"Episode 1/20 end after 48600 steps -Reward: -161.15499639790505 -Epsilon: 0.772875023859198\nEpisode 2/20 end after 48338 steps -Reward: -159.26999644748867 -Epsilon: 0.6650209605890582\nEpisode 3/20 end after 48498 steps -Reward: -153.7849969258532 -Epsilon: 0.5730391665408499\nEpisode 4/20 end after 48301 steps -Reward: -151.8899968545884 -Epsilon: 0.49484371407380506\nEpisode 5/20 end after 48481 steps -Reward: -163.49499634932727 -Epsilon: 0.42861367176132725\nEpisode 6/20 end after 48383 steps -Reward: -159.66999643296003 -Epsilon: 0.3724548144384288\nEpisode 7/20 end after 48596 steps -Reward: -157.49499660078436 -Epsilon: 0.3242288608008147\nEpisode 8/20 end after 48600 steps -Reward: -162.8199963606894 -Epsilon: 0.28321552269914074\nEpisode 9/20 end after 48599 steps -Reward: -164.17999633401632 -Epsilon: 0.2483360900417373\nEpisode 10/20 end after 48600 steps -Reward: -166.4299962799996 -Epsilon: 0.21867318331889518\nEpisode 11/20 end after 48156 steps -Reward: -135.63499745633453 -Epsilon: 0.19361982193077437\nEpisode 12/20 end after 46452 steps -Reward: -65.86500059533864 -Epsilon: 0.1725929702336384\nEpisode 13/20 end after 48600 steps -Reward: -163.72999634034932 -Epsilon: 0.15425811327366035\nEpisode 14/20 end after 48459 steps -Reward: -154.31999667175114 -Epsilon: 0.1386653954355438\nEpisode 15/20 end after 48600 steps -Reward: -165.2649963060394 -Epsilon: 0.12540470569523993\nEpisode 16/20 end after 48569 steps -Reward: -156.30999662727118 -Epsilon: 0.11412726873946162\nEpisode 17/20 end after 48600 steps -Reward: -159.15499644260854 -Epsilon: 0.10453647167066296\nEpisode 18/20 end after 48431 steps -Reward: -156.58499662112445 -Epsilon: 0.09638006265897289\nEpisode 19/20 end after 48424 steps -Reward: -160.07499654311687 -Epsilon: 0.08944351635434838\nEpisode 20/20 end after 48600 steps -Reward: -159.29999643936753 -Epsilon: 0.08354439155969443\nModel saved as 'blue_agent.pt'\n","output_type":"stream"}],"execution_count":16}]}