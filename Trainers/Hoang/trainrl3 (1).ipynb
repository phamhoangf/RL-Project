{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":9835.268821,"end_time":"2024-11-29T15:18:37.899348","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-11-29T12:34:42.630527","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/Farama-Foundation/MAgent2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AsQFcJrPsSUb","outputId":"d60498a9-972a-4c1d-b9b1-11a2d2b35f02","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:06.800016Z","iopub.execute_input":"2024-12-12T17:22:06.800514Z","iopub.status.idle":"2024-12-12T17:22:47.737628Z","shell.execute_reply.started":"2024-12-12T17:22:06.800463Z","shell.execute_reply":"2024-12-12T17:22:47.736501Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/Farama-Foundation/MAgent2\n  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-lbygbq3l\n  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-lbygbq3l\n  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.26.4)\nCollecting pygame>=2.1.0 (from magent2==0.3.3)\n  Downloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nRequirement already satisfied: pettingzoo>=1.23.1 in /opt/conda/lib/python3.10/site-packages (from magent2==0.3.3) (1.24.0)\nRequirement already satisfied: gymnasium>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (0.29.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\nDownloading pygame-2.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: magent2\n  Building wheel for magent2 (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for magent2: filename=magent2-0.3.3-cp310-cp310-linux_x86_64.whl size=1657467 sha256=e19c8dca95a7813d465d3fd6ec98e39413f6ea76debdc77e97fb9aa2010bb52c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-ivdx7via/wheels/e4/8e/bf/51a30bc4038546e23b81c9fb513fe6a8fd916e5a9c5f4291d5\nSuccessfully built magent2\nInstalling collected packages: pygame, magent2\nSuccessfully installed magent2-0.3.3 pygame-2.6.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Import","metadata":{"collapsed":false,"id":"V6K8st1DsSUe","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport numpy as np\nimport os\nfrom tqdm import tqdm\n\nfrom magent2.environments import battle_v4\nimport cv2\nfrom collections import deque\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"po2mzBoFsSUg","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:47.739671Z","iopub.execute_input":"2024-12-12T17:22:47.740045Z","iopub.status.idle":"2024-12-12T17:22:51.269184Z","shell.execute_reply.started":"2024-12-12T17:22:47.740004Z","shell.execute_reply":"2024-12-12T17:22:51.268056Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## DQN","metadata":{"collapsed":false,"id":"qsvHiL9rsSUk","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class QNetwork(nn.Module):\n    def __init__(self, observation_shape, action_shape, device='cpu'):\n        super().__init__()\n        self.observation_shape = observation_shape\n        self.action_shape = action_shape\n        self.device = device\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], kernel_size=3),\n            nn.ReLU(),\n            nn.Conv2d(observation_shape[-1], observation_shape[-1], kernel_size=3),\n            nn.ReLU(),\n        )\n\n        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n        dummy_output = self.cnn(dummy_input)\n        flatten_dim = dummy_output.view(-1).shape[0]\n        self.network = nn.Sequential(\n            nn.Linear(flatten_dim, 120),\n            nn.ReLU(),\n            nn.Linear(120, 84),\n            nn.ReLU(),\n            nn.Linear(84, action_shape),\n        )\n\n    def forward(self, x):\n        assert len(x.shape) >= 3, \"only support magent input observation\"\n        x = self.cnn(x)\n        if len(x.shape) == 3:\n            batchsize = 1\n        else:\n            batchsize = x.shape[0]\n        x = x.reshape(batchsize, -1)\n        return self.network(x)","metadata":{"id":"4uWvLuKvsSUk","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.270415Z","iopub.execute_input":"2024-12-12T17:22:51.270836Z","iopub.status.idle":"2024-12-12T17:22:51.280374Z","shell.execute_reply.started":"2024-12-12T17:22:51.270803Z","shell.execute_reply":"2024-12-12T17:22:51.279069Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Replay Buffer","metadata":{"collapsed":false,"id":"MgdKXxwCsSUh","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class ReplayMemory(Dataset):\n    def __init__(self, maxlen):\n        super().__init__()\n        self.maxlen = maxlen\n        self.step_memory = [deque([],maxlen=self.maxlen)]\n\n    def push(self, step_idx, observation, action, reward, next_observation, done):\n        if step_idx == len(self.step_memory):\n            self.step_memory.append(deque([],maxlen=self.maxlen))\n        self.step_memory[step_idx].append((observation, action, reward, next_observation, done))\n\n    def __len__(self):\n        return sum([len(memory) for memory in self.step_memory])\n\n    def __getitem__(self, idx):\n        step_idx = 0\n        while idx >= len(self.step_memory[step_idx]):\n            idx -= len(self.step_memory[step_idx])\n            step_idx += 1\n        observation, action, reward, next_observation, done = self.step_memory[step_idx][idx]\n        return (\n            torch.Tensor(observation).float().permute([2, 0, 1]),\n            torch.tensor(action),\n            torch.tensor(reward, dtype=torch.float),\n            torch.tensor(next_observation).float().permute([2,0,1]),\n            torch.tensor(done, dtype=torch.float32),\n        )","metadata":{"id":"Lr6mxiaZsSUi","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.283168Z","iopub.execute_input":"2024-12-12T17:22:51.283646Z","iopub.status.idle":"2024-12-12T17:22:51.300777Z","shell.execute_reply.started":"2024-12-12T17:22:51.283590Z","shell.execute_reply":"2024-12-12T17:22:51.299812Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Trainer","metadata":{"collapsed":false,"id":"_DOd5Mg-sSUl","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"class Trainer:\n    def __init__(\n        self,\n        policy_dqn, target_dqn,\n        n_action,\n        loss_fn, optimizer, scheduler,\n        epsilon_start, epsilon_end, epsilon_decay,\n        device='cpu'\n    ):\n\n        self.policy_dqn = policy_dqn.to(device)\n        self.target_dqn = target_dqn.to(device)\n        self.target_dqn.eval()\n\n        self.n_action = n_action\n\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n        self.epsilon_start = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n        self.epsilon = self.epsilon_start\n\n        self.device = device\n\n        self.policy_dqn.apply(self.weights_init)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n            nn.init.xavier_uniform_(m.weight)\n            if torch.is_tensor(m.bias):\n                m.bias.data.fill_(0.01)\n\n\n    def policy(self, observation):\n        if np.random.rand() < self.epsilon:\n            return np.random.randint(0, self.n_action)\n        else:\n            with torch.no_grad():\n                q_values = self.policy_dqn(\n                    torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(self.device)\n                )\n            return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n\n\n    def optimize_model(self, replay_memory, batch_size, gamma):\n        if len(replay_memory) < batch_size:\n            return\n        train_loader = DataLoader(replay_memory, batch_size=batch_size, shuffle=True)\n        self.policy_dqn.train()\n\n        for observations, actions, rewards, next_observations, dones in train_loader:\n\n            self.policy_dqn.zero_grad()\n\n            observations = observations.to(self.device)\n            actions = actions.unsqueeze(1).to(self.device)\n            rewards = rewards.unsqueeze(1).to(self.device)\n            next_observations = next_observations.to(self.device)\n            dones = dones.unsqueeze(1).to(self.device)\n\n            current_q_values = self.policy_dqn(observations).gather(1, actions)\n\n            with torch.no_grad():\n                target_q_values = rewards + gamma * (1 - dones) * self.target_dqn(next_observations).max(1, keepdim=True)[0]\n\n            # loss\n            loss = self.loss_fn(current_q_values, target_q_values)\n\n            loss.backward()\n            self.optimizer.step()\n            self.scheduler.step()\n\n    def train(self,\n              env, episodes,\n              target_agent, batch_size, gamma, replay_memory,\n              update_tg_freq, TAU\n             ):\n        train_rewards = []\n        train_durations = []\n\n        for episode in tqdm(range(episodes)):\n            ep_reward = 0\n\n            ep_steps = 0\n\n            observations = {}\n            actions = {}\n            step_idx = {}\n\n            env.reset()\n\n            for idx, agent in enumerate(env.agent_iter()):\n                ep_steps += 1\n                observation, reward, termination, truncation, info = env.last()\n\n                if target_agent in agent:\n                    ep_reward += reward\n                else:\n                    ep_reward -= reward\n\n                action = self.policy(observation)\n                \n                step_idx[agent] = 0\n                observations[agent] = observation\n                actions[agent] = action\n                env.step(action)\n\n                if (idx+1) % env.num_agents == 0:\n                    break\n\n            for agent in env.agent_iter():\n                ep_steps += 1\n\n                next_observation, reward, termination, truncation, info = env.last()\n\n                if target_agent in agent:\n                    ep_reward += reward\n                else:\n                    ep_reward -= reward\n\n                # Agent die\n                if termination or truncation:\n                    action = None\n                else:\n                    action = self.policy(next_observation)\n\n                replay_memory.push(\n                    step_idx[agent],\n                    observations[agent],\n                    actions[agent],\n                    reward,\n                    next_observation,\n                    termination\n                )\n\n                step_idx[agent] += 1\n                observations[agent] = next_observation\n                actions[agent] = action\n                env.step(action)\n\n            # Training\n            self.optimize_model(replay_memory, batch_size, gamma)\n\n            if episode % update_tg_freq == 0:\n                target_dqn_state_dict = self.target_dqn.state_dict()\n                policy_dqn_state_dict = self.policy_dqn.state_dict()\n                for key in policy_dqn_state_dict:\n                    target_dqn_state_dict[key] = policy_dqn_state_dict[key]*TAU + target_dqn_state_dict[key]*(1-TAU)\n                self.target_dqn.load_state_dict(target_dqn_state_dict)\n\n\n            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n\n            print(f\"\\nEpisode {episode + 1}, Episode Reward: {ep_reward}, Steps: {ep_steps}, Epsilon: {self.epsilon}\")\n\n            train_rewards.append(ep_reward)\n            train_durations.append(ep_steps)\n\n        return train_rewards, train_durations","metadata":{"id":"LmFWyL_usSUl","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.302399Z","iopub.execute_input":"2024-12-12T17:22:51.302919Z","iopub.status.idle":"2024-12-12T17:22:51.326731Z","shell.execute_reply.started":"2024-12-12T17:22:51.302868Z","shell.execute_reply":"2024-12-12T17:22:51.325443Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Config","metadata":{"id":"xWMfYGAdsSUn"}},{"cell_type":"code","source":"\nenv = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n\nepisodes = 40\ntarget_agent = 'blue'\nbatch_size = 1024\ngamma = 0.9\nupdate_tg_freq = 1\nTAU = 0.3\n\nmaxlen = 81 * episodes\n\nlearning_rate = 1e-3\ntheta = 1e-6\nepsilon_start = 1\nepsilon_end = 0.01\nepsilon_decay = 0.9\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"id":"O-LNgyXrsSUo","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.328462Z","iopub.execute_input":"2024-12-12T17:22:51.329389Z","iopub.status.idle":"2024-12-12T17:22:51.372720Z","shell.execute_reply.started":"2024-12-12T17:22:51.329298Z","shell.execute_reply":"2024-12-12T17:22:51.371630Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def seed_everything(seed_value):\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = True\n\nseed_everything(42)","metadata":{"id":"ph8bg0yUsSUp","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.374032Z","iopub.execute_input":"2024-12-12T17:22:51.374354Z","iopub.status.idle":"2024-12-12T17:22:51.384812Z","shell.execute_reply.started":"2024-12-12T17:22:51.374320Z","shell.execute_reply":"2024-12-12T17:22:51.383729Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Training Loop","metadata":{"collapsed":false,"id":"bsMx4KkzsSUp","jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"policy_dqn = QNetwork(\n    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n\ntarget_dqn = QNetwork(\n    env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n    )\n\ntarget_dqn.load_state_dict(policy_dqn.state_dict())","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hg8KvprxsSUr","outputId":"a26ed9cc-b23c-4dbe-a3a6-cfd00ba00b0d","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.386132Z","iopub.execute_input":"2024-12-12T17:22:51.386519Z","iopub.status.idle":"2024-12-12T17:22:51.497627Z","shell.execute_reply.started":"2024-12-12T17:22:51.386471Z","shell.execute_reply":"2024-12-12T17:22:51.496504Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"loss_function = nn.MSELoss()","metadata":{"id":"dzXr2CaLsSUs","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.498696Z","iopub.execute_input":"2024-12-12T17:22:51.499028Z","iopub.status.idle":"2024-12-12T17:22:51.503586Z","shell.execute_reply.started":"2024-12-12T17:22:51.498964Z","shell.execute_reply":"2024-12-12T17:22:51.502540Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(policy_dqn.parameters(), weight_decay=0, lr=learning_rate)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=episodes, eta_min=theta)","metadata":{"id":"TFjDRP6iG9ky","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:51.506431Z","iopub.execute_input":"2024-12-12T17:22:51.506842Z","iopub.status.idle":"2024-12-12T17:22:52.542430Z","shell.execute_reply.started":"2024-12-12T17:22:51.506808Z","shell.execute_reply":"2024-12-12T17:22:52.541418Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"replay_memory = ReplayMemory(maxlen)","metadata":{"id":"tZd9oUAlsSUq","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:52.543944Z","iopub.execute_input":"2024-12-12T17:22:52.544415Z","iopub.status.idle":"2024-12-12T17:22:52.548620Z","shell.execute_reply.started":"2024-12-12T17:22:52.544381Z","shell.execute_reply":"2024-12-12T17:22:52.547571Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\ntrainer = Trainer(\n    policy_dqn, target_dqn,\n    env.action_space(\"red_0\").n,\n    loss_function, optimizer, lr_scheduler,\n    epsilon_start, epsilon_end, epsilon_decay,\n    device=device\n)","metadata":{"id":"JuZgCRyCsSUs","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:52.550052Z","iopub.execute_input":"2024-12-12T17:22:52.550479Z","iopub.status.idle":"2024-12-12T17:22:52.574434Z","shell.execute_reply.started":"2024-12-12T17:22:52.550433Z","shell.execute_reply":"2024-12-12T17:22:52.573308Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ntrain_rewards, train_durations = trainer.train(\n    env, episodes,\n    target_agent, batch_size, gamma, replay_memory,\n    update_tg_freq, TAU\n)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3INdaN92sSUt","outputId":"644786ef-c42f-4863-c164-0d523cc15757","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T17:22:52.575827Z","iopub.execute_input":"2024-12-12T17:22:52.576800Z"}},"outputs":[{"name":"stderr","text":"  2%|▎         | 1/40 [00:36<23:53, 36.75s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 1, Episode Reward: 65.33500276319683, Steps: 158611, Epsilon: 0.9\n","output_type":"stream"},{"name":"stderr","text":"  5%|▌         | 2/40 [01:58<39:58, 63.13s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 2, Episode Reward: 1.4350003516301513, Steps: 160308, Epsilon: 0.81\n","output_type":"stream"},{"name":"stderr","text":"  8%|▊         | 3/40 [03:41<50:12, 81.42s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 3, Episode Reward: -122.04000551160425, Steps: 103534, Epsilon: 0.7290000000000001\n","output_type":"stream"},{"name":"stderr","text":" 10%|█         | 4/40 [05:28<54:55, 91.56s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 4, Episode Reward: 42.000003438442945, Steps: 53125, Epsilon: 0.6561000000000001\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▎        | 5/40 [07:39<1:01:40, 105.74s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 5, Episode Reward: 157.2750077350065, Steps: 88321, Epsilon: 0.5904900000000002\n","output_type":"stream"},{"name":"stderr","text":" 15%|█▌        | 6/40 [10:23<1:11:04, 125.42s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 6, Episode Reward: -149.68000826239586, Steps: 123226, Epsilon: 0.5314410000000002\n","output_type":"stream"},{"name":"stderr","text":" 18%|█▊        | 7/40 [12:59<1:14:31, 135.51s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 7, Episode Reward: 32.19000013452023, Steps: 36241, Epsilon: 0.47829690000000014\n","output_type":"stream"},{"name":"stderr","text":" 20%|██        | 8/40 [15:42<1:16:52, 144.15s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 8, Episode Reward: 5.854999823495746, Steps: 36305, Epsilon: 0.43046721000000016\n","output_type":"stream"},{"name":"stderr","text":" 22%|██▎       | 9/40 [18:37<1:19:33, 153.97s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 9, Episode Reward: 300.07001288980246, Steps: 51952, Epsilon: 0.38742048900000015\n","output_type":"stream"},{"name":"stderr","text":" 25%|██▌       | 10/40 [21:29<1:19:44, 159.48s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 10, Episode Reward: -40.7999964999035, Steps: 25440, Epsilon: 0.34867844010000015\n","output_type":"stream"},{"name":"stderr","text":" 28%|██▊       | 11/40 [24:22<1:19:01, 163.51s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 11, Episode Reward: -28.305002064444125, Steps: 14793, Epsilon: 0.31381059609000017\n","output_type":"stream"},{"name":"stderr","text":" 30%|███       | 12/40 [27:18<1:18:08, 167.43s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 12, Episode Reward: -50.61500302888453, Steps: 19536, Epsilon: 0.28242953648100017\n","output_type":"stream"},{"name":"stderr","text":" 32%|███▎      | 13/40 [30:18<1:17:02, 171.21s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 13, Episode Reward: -16.59500029589981, Steps: 15925, Epsilon: 0.25418658283290013\n","output_type":"stream"},{"name":"stderr","text":" 35%|███▌      | 14/40 [33:17<1:15:11, 173.53s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 14, Episode Reward: -57.514999899081886, Steps: 9011, Epsilon: 0.22876792454961012\n","output_type":"stream"},{"name":"stderr","text":" 38%|███▊      | 15/40 [36:18<1:13:14, 175.79s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 15, Episode Reward: 61.029998858459294, Steps: 13221, Epsilon: 0.2058911320946491\n","output_type":"stream"},{"name":"stderr","text":" 40%|████      | 16/40 [39:18<1:10:50, 177.12s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 16, Episode Reward: 174.23999799694866, Steps: 7138, Epsilon: 0.1853020188851842\n","output_type":"stream"},{"name":"stderr","text":" 42%|████▎     | 17/40 [42:20<1:08:24, 178.46s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 17, Episode Reward: 78.01999875437468, Steps: 11148, Epsilon: 0.16677181699666577\n","output_type":"stream"},{"name":"stderr","text":" 45%|████▌     | 18/40 [45:25<1:06:12, 180.58s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 18, Episode Reward: 128.16499777231365, Steps: 16232, Epsilon: 0.1500946352969992\n","output_type":"stream"},{"name":"stderr","text":" 48%|████▊     | 19/40 [48:35<1:04:07, 183.24s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 19, Episode Reward: -77.41499787848443, Steps: 20579, Epsilon: 0.13508517176729928\n","output_type":"stream"},{"name":"stderr","text":" 50%|█████     | 20/40 [51:46<1:01:52, 185.63s/it]","output_type":"stream"},{"name":"stdout","text":"\nEpisode 20, Episode Reward: -50.55999910645187, Steps: 14124, Epsilon: 0.12157665459056936\n","output_type":"stream"},{"name":"stderr","text":" 52%|█████▎    | 21/40 [54:59<59:28, 187.80s/it]  ","output_type":"stream"},{"name":"stdout","text":"\nEpisode 21, Episode Reward: -72.33999922219664, Steps: 15292, Epsilon: 0.10941898913151243\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"trainer.policy_dqn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"duJaLq6LsSUt","outputId":"3921aef2-f425-4a7f-eae8-45173a7dcd68","scrolled":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(trainer.policy_dqn.state_dict(), 'blue.pt')","metadata":{"id":"xq5eaQO5sSUt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n# Vẽ cả 2 mảng trên cùng 1 biểu đồ\nplt.plot(train_rewards, label='Train Rewards', marker='o')\nplt.plot(train_durations, label='Train Durations', marker='x')\nplt.title('Train Rewards and Train Durations')\nplt.xlabel('Episode')\nplt.ylabel('Value')\nplt.legend()  # Hiển thị chú thích\nplt.grid(True)  # Hiển thị lưới\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}